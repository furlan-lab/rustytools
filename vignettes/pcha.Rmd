---
title: "PCHA"
output:
  html_document:
    self_contained: true
    mathjax: default
date: "2025-05-06"
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

<!-- load MathJax from CDN -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>


## Overview of PCHA implemented in Rust


### 1.  Problem set-up

| symbol                       | shape               | meaning                           |        |                              |
| ---------------------------- | ------------------- | --------------------------------- | ------ | ---------------------------- |
| $X\in\mathbb{R}^{p\times n}$ | variables × samples | raw data                          |        |                              |
| $I\subseteq\{1,\dots,n\}$    | (I=n\_I)            | dictionary columns (“atoms”)      |        |
| $U\subseteq\{1,\dots,n\}$    | (U=n\_U)            | columns to be approximated        |        |
| $k$                          | —                   | number of archetypes (components) |        |                              |

We split $X = [\,X_I\;X_{\,\bar I}\,]$ and seek

$$
\boxed{\;X_U\;\approx\;\underbrace{X_I}_{p\times n_I}\;
        \underbrace{C}_{n_I\times k}\;
        \underbrace{S}_{k\times n_U}\;}
$$

with the **convex-hull constraints**

$$
C\ge 0,\quad S\ge 0,\qquad 
\|S_{:,j}\|_{1}=1,\quad 
\|C_{:,t}\|_{1}\approx 1\;(\pm\delta).
$$

The **sum-of-squares error** (SSE) to minimise is

$$
\mathrm{SSE}(C,S)=
\|X_U-X_I C S\|_F^{2}.
\tag{1}
$$

---

### 2.  Algebra that drives the code

1. **Expand (1)**

   $$
   \begin{aligned}
   \|A-B\|_F^{2}
     &=\operatorname{tr}\bigl((A-B)^\top(A-B)\bigr) \\
     &=\|X_U\|_F^{2}
       -2\,\operatorname{tr}\!\bigl(S^\top C^\top X_I^\top X_U\bigr)
       +\operatorname{tr}\!\bigl(S^\top C^\top X_I^\top X_I C S\bigr).
   \end{aligned}
   $$

   Two cached Gram matrices therefore appear everywhere in the Rust code

   $$
   G := X_I^\top X_U,\qquad
   H := X_I^\top X_I.
   $$

2. **Gradients**

   $$
   \nabla_S\mathrm{SSE} = 
   (C^\top H C)\,S - C^\top G,
   \quad
   \nabla_C\mathrm{SSE} = 
   H\,C\,\bigl(SS^\top\bigr)-G S^\top .
   $$

   (These two lines are the heart of `s_update` and `c_update`.)

3. **Projected–gradient step**

   * **Take a gradient step:**
     $Z\leftarrow Y - \mu\,\nabla_Y$  for $Y\in\{C,S\}$.

   * **Project back to the feasible set.**
     Each column is sent onto the probability simplex

     $$
       \Delta^{m-1}:=\Bigl\{\,z\in\mathbb{R}^m_{\ge 0}:
            \langle\mathbf 1,z\rangle=1\Bigr\},
     $$

     using the Wang–Carreira-Perpiñán algorithm (O(m log m)), implemented
     in `project_simplex`.

   * **Armijo-type line-search**
     If the new SSE is not smaller, shrink $\mu:=\mu/2$;
     else accept and enlarge $\mu:=1.2\,\mu$.
     (see the inner `while !stop` loops).

4. **Optional “α-relaxation’’**
   To let each archetype deviate slightly from exact $ \ell_1\! =\!1$,
   a per-column scalar $\alpha_t\in[1-\delta,\,1+\delta]$ is fitted
   (second part of `c_update`).
   In formulas: minimise w\.r.t. $\alpha$

   $$
     \|\,X_U - X_I\,(C\operatorname{diag}\alpha)\,S\|_F^{2}.
   $$

---

### 3.  Initialisation

* **Furthest-Sum (FS)** finds $k$ columns of $X_I$ that are mutually
  far apart in Euclidean distance:

  ```
  choose seed  i₁
  repeat
        pick   i_{t+1} := argmax_j Σ_{s≤t} ‖x_{i_s}-x_j‖
  ```

  FS provides an indicator matrix $C^{(0)}$ with **exact** simplex
  columns (one “1” each).

* **Dirichlet-type random $S^{(0)}$**
  Each column is drawn i.i.d.\ from $\operatorname{Dirichlet}(\mathbf 1)$
  ⇒ uniform over the simplex.

---

### 4.  Stopping rule

Iterate until

$$
\frac{|\mathrm{SSE}^{(t-1)}-\mathrm{SSE}^{(t)}|}
           {\mathrm{SSE}^{(t)}}
     <\text{tol} \quad\text{or}\quad
     \frac{\mathrm{SSE}^{(t)}}{\|X_U\|_F^{2}}
     > 1-\varepsilon\;(\text{≈ 99.99 % var. expl.})
$$

or the iteration cap is hit (default 750).

---

### 5.  Output

* $C$ and $S$ re-ordered by column usage
  $u_t=\sum_j S_{tj}$ (desc).
* Archetypes $XC = X_I C$.
* Final SSE and variance explained
  $\mathrm{VarExpl}=1-\mathrm{SSE}/\|X_U\|_F^{2}$.

---




## Create a data set with a known archetypal structure

```{r}
# library(remotes)
# install_github("furlan-lab/rustytools", force = T)

library(rustytools)
### Toy data with a known archetypal structure
set.seed(42)
p  <- 100            # features
n  <- 3000           # cases
k0 <- 5             # ground-truth archetypes

# 1. ground-truth archetype matrix A (p × k0)d
A <- matrix(rexp(p * k0, 1), p, k0)
# 2. sample coefficients S on the simplex
alpha <- matrix(rexp(k0 * n, 1), k0, n)
S     <- sweep(alpha, 2, colSums(alpha), "/")
# 3. generate data with small noise
X <- A %*% S + matrix(rgamma(p * n, shape = 1, rate = 50), p, n)

```

# Finding optimal number of archetypes

First we set a number of k to iterate through.  Using the future library we can perform pcha on each k in parallel.  We normalize the sum squared error, then use the `find_knee_pt` function to find the optimal number of archetypes which is our ground truth archetype from above

```{r}

ks <- 3:8
microbenchmark::microbenchmark(sse <- sapply(ks, function(k) pcha(X, k,)$sse), times = 1)
microbenchmark::microbenchmark(sse <- sapply(ks, function(k) pcha(X, k, num_cores = 8)$sse), times = 1)
```

```{r}
norm_sse  <- sse / sum(X * X)                           
kp   <- find_knee_pt(norm_sse, ks, make_plot = TRUE)
cat("Optimal number of archetypes =", kp$knee_x, "\n")
```


We can compare the performance of our rust implementation of pcha to that implemented in the paretoTI package (note that this uses python and requires reticulate).  We see a slight gain in performance (Even a slight loss would still be worth it not to have to deal with python in R)...  Although Rust implementation could be optimized further by using openblas libraries, this results in a variety of compilation issues across different devices.
```{r}
library(reticulate)
use_python("/Users/sfurlan/.virtualenvs/reticulate/bin/python")
suppressWarnings(suppressPackageStartupMessages({library(ParetoTI)}))
microbenchmark::microbenchmark(sse <- sapply(ks, function(k) {
  res = py_PCHA$PCHA(X, noc = as.integer(k), conv_crit = 1e-6, maxiter = 750)
  res[[4]]}), times=3)
```

```{r}
norm_sse  <- sse / sum(X * X)                           
kp   <- find_knee_pt(norm_sse, ks, make_plot = TRUE)
cat("Optimal number of archetypes =", kp$knee_x, "\n")
```

## T ratio

To obtain the t-ratio we can run the pcha algorithm using the argument calc_t_ratio set to true (is false by default).  The calculation of t-ratio is done in R and therefore not including in the Rust/Python bakeoff above.  Performance gains implementing the t-ratio in Rust were not realized.
```{r}
library(ggplot2)
tratios <- sapply(ks, function(k) pcha(X, k, calc_t_ratio = T)$t_ratio)

ggplot(data.frame(ks = ks, tratios=tratios), aes(x=ks, y=tratios))+
  geom_line() +
  geom_point()+theme_bw() + labs(x = "Number of archetypes (k)", y = "T ratio") 
```

We see a maximum t-ratio at 5, as expected.  To ensure robustness we have implemented similar to that proposed by Korem Y, et. al. in "Geometry of the Gene Expression Space of
Individual Cells" PLOS Computational Biology 2015.


```{r}


```

## What does this look like in single cell RNA sequencing data

First subset out the tumor
```{r}
library(Seurat)
library(ggplot2)
library(scCustomize)
library(magrittr)

seu <- readRDS("~/Fred Hutch Cancer Center/Furlan_Lab - General/experiments/patient_marrows/annon/AML101/aml101.cds")
DimPlot(seu, group.by = "seurat_clusters")
DimPlot(seu, group.by = "geno")
seu$sb <- seu$geno %in% "0" & seu$seurat_clusters %in% c("0", "1", "2", "3", "4", "11")
seu <- seu[,seu$sb]
seu <- NormalizeData(seu, verbose = F) %>% ScaleData(verbose = F) %>% FindVariableFeatures(verbose = F) %>% RunPCA(npcs = 100, verbose = F)
ElbowPlot(seu, ndims = 100)
seu <- FindNeighbors(seu, dims = 1:35, verbose = F) %>% FindClusters(verbose = F) %>% RunUMAP(dims = 1:35, n.epochs = 500, verbose = F)
DimPlot(seu)

```

Use some magic.

```{r}

seu <- seurat_magic(seu, alpha = 1)
DefaultAssay(seu) <- "MAGIC"
seu <- NormalizeData(seu, assay = "MAGIC") %>% ScaleData(verbose = F) %>% FindVariableFeatures(nfeatures = 5000, assay = "MAGIC", verbose = F) %>%  RunPCA(assay = "MAGIC", npcs = 100, verbose = F)
```


Find nuber of PCs that correspond to 85% of variance
```{r}
# pull out the per‐PC standard deviations (as before)
sdev <- seu@reductions$pca@stdev

# compute the % variance explained by each PC
explained_var <- sdev^2 / sum(sdev^2)

# cumulative sum
cum_var <- cumsum(explained_var)

# find the first PC where cumulative ≥ 0.85
pct_cutoff <- 0.85
n_pc_85 <- which(cum_var >= pct_cutoff)[1]

message("Using ", n_pc_85, " PCs, which explain ",
        round(100 * cum_var[n_pc_85], 1), "% of variance.\n")

# now subset your cell×PC matrix
pcs_85 <- Embeddings(seu, "pca")[, 1:n_pc_85, drop = FALSE]

seu <- FindNeighbors(seu, dims = 1:n_pc_85, verbose = F, k.param = 5) %>% FindClusters(verbose = F) %>% RunUMAP(dims = 1:n_pc_85, n.epochs = 500, verbose = F)
DimPlot(seu, reduction = "umap")

X <- as.matrix(t(seu@reductions$pca@cell.embeddings[,1:n_pc_85]))
```

```{r, include = F, eval=F}
#saveRDS(X, "temp.RDS")
X<-readRDS("temp.RDS")
```

## Reproducibility

To ensure reproducibility we can pass random starts to the rust pcha implementation from R.  Note that this is not necessary to do in normal practice.  Because slightly different solutions may be found with each run of pcha given the random starts, using R to generate random starts is one way to ensure reproducibility.  However, the random starts produced in Rust are adequate for most purposes.  One suggestion would be to calculate multiple solutions using Rust random starts and use a consensus solution.
```{r}
set.seed(123)

# ---- 1. implement furthest_sum in R, this is done in the Rust implementation natively ----
furthest_sum <- function(X, k, seed = NULL){
  # X: p×n data matrix (genes × cells), but we only care about columns here
  n <- ncol(X)
  # precompute squared norms
  norms2 <- colSums(X^2)
  # pick initial seed at random if not provided
  if (is.null(seed)) seed <- sample.int(n, 1)
  archetypes <- integer(k)
  archetypes[1] <- seed
  # maintain a running "sum of distances" for each column
  sum_dists <- numeric(n)
  picked <- logical(n)
  picked[seed] <- TRUE

  for (j in 2:k) {
    last <- archetypes[j - 1]
    # distances from last seed to all columns: sqrt(||x_i||^2 + ||x_last||^2 - 2 x_i·x_last)
    # we only need the sqrt to compare, but sum of sqrt is monotonic so we'll do it exactly:
    dots <- crossprod(X[, last], X)    # 1×n vector of dot products
    d2   <- norms2 + norms2[last] - 2 * as.numeric(dots)
    sum_dists[!picked] <- sum_dists[!picked] + sqrt(d2[!picked])
    # pick the column with max cumulative distance
    next_seed <- which.max(ifelse(picked, -Inf, sum_dists))
    archetypes[j] <- next_seed
    picked[next_seed] <- TRUE
  }
  archetypes
}


# ---- 2. build C0 and S0 ----
make_pcha_starts <- function(X, k, rng_seed = 1234, s_rng_seed = 5678){
  set.seed(rng_seed)
  seeds <- furthest_sum(X, k)
  # C0: n×k (because Rust's ni = #columns of X)
  C0 <- matrix(0, nrow = ncol(X), ncol = k)
  for (j in seq_len(k)) C0[seeds[j], j] <- 1

  # S0: k×n uniform → normalized columns
  set.seed(s_rng_seed)
  S0 <- matrix(runif(k * ncol(X)), nrow = k, ncol = ncol(X))
  S0 <- sweep(S0, 2, colSums(S0), "/")

  list(C0 = C0, S0 = S0)
}

kmax <- 8
ks  <- 2:kmax
starts <- lapply(ks, function(k){
  make_pcha_starts(X, k,
                   rng_seed   = 1000 + k,
                   s_rng_seed = 2000 + k)
})

res  <- sapply(seq_along(ks), function(i){
  k  <- ks[i]
  C0 <- starts[[i]]$C0
  S0 <- starts[[i]]$S0
  message("running PCHA for k=", k, " …")
  res = pcha(X, k, c_init = C0, s_init = S0, calc_t_ratio = T)
  return(list(sse=res$sse, t_ratio = res$t_ratio))
})

norm_sse  <- sse / sum(X * X)                           
kp   <- find_knee_pt(norm_sse, ks, make_plot = TRUE)

cat("Optimal number of archetypes =", kp$knee_x, "\n")

noc <- kp$knee_x
res <- pcha(X, noc) 
weights_df <- as.data.frame(t(res$S))     # cells × k
colnames(weights_df) <- paste0("arch", seq_len(noc))


# a single label: archetype with the largest weight
seu$max_arch <- factor(
  apply(weights_df, 1L, which.max),
  levels = seq_len(noc),
  labels = paste0("Arch", seq_len(noc))
)

DimPlot(seu, reduction = "umap", group.by = "max_arch")


S_df <- as.data.frame(t(res$S))
colnames(S_df) <- paste0("Arch", seq_len(ncol(S_df)))

# 2) pick your “specialist” threshold
specialist_thresh <- 0.95


# 3) for each cell, find if its max-weight > threshold; if so, label by archetype
max_w   <- apply(S_df, 1, max)
best_k  <- apply(S_df, 1, which.max)
specialist_label <- ifelse(max_w > specialist_thresh,
                           paste0("Arch", best_k),
                           NA_character_)

# 4) add to Seurat
seu$specialist <- specialist_label


p <- DimPlot(seu, reduction = "umap", group.by = "specialist")
p + scale_colour_discrete(na.value = "lightgrey")


```



## T ratio
```{r}
library(geometry)   # for convhulln()


simplex_volume <- function(verts){
  # verts: d × k matrix of k vertices in R^d
  k <- ncol(verts)
  if(k < 2) return(0)
  M <- verts[, -1, drop=FALSE] - verts[, 1]
  Gm <- crossprod(M)             # Gram matrix (k-1)×(k-1)
  sqrt(abs(det(Gm))) / factorial(k-1)
}

compute_t_ratio <- function(X, k, ..., centre = TRUE){
  # X: d×n (e.g. your top‐12‐PCs matrix)
  res   <- pcha(X, k, ...)
  XC    <- res$XC   # d×k
  if(centre){
    X  <- X  - rowMeans(X)
    XC <- XC - rowMeans(XC)
  }
  V_poly  <- simplex_volume(XC)
  hull_all <- convhulln(t(X), options="FA")
  V_hull  <- hull_all$vol
  list(t_ratio     = V_poly / V_hull,
       polytopeVol = V_poly,
       hullVol     = V_hull,
       pcha        = res)
}


obs5 <- compute_t_ratio(X, 5, max_iter = 20, conv_crit = 1e-2)
print(obs5$t_ratio)

# 3) null distribution by shuffling columns of X:
set.seed(123)
B <- 1000
null_tr <- replicate(B, {
  X_shuf <- pc_mat[, sample(ncol(pc_mat))]
  compute_t_ratio(X_shuf, 5, max_iter=100, conv_crit=1e-3)$t_ratio
})

pval <- mean(null_tr >= obs5$t_ratio)
cat("Empirical p-value for k=5:", pval, "\n")




```


## Pathway analysis of archetype specialists


```{r}
suppressPackageStartupMessages({library(dplyr)
library(purrr)
library(clusterProfiler)
library(org.Hs.eg.db)})

DefaultAssay(seu)<- "RNA"
seu_spec <-seu[,!is.na(seu$specialist)]
# 1) For each archetype, find a “marker gene list” (here: top 200 avg. expr)
marker_genes <- FindAllMarkers(seu_spec, group.by = "specialist", )
marker_genes$diff <- abs(marker_genes$pct.1 - marker_genes$pct.2)

marker_genes_filt <- marker_genes[marker_genes$p_val_adj<5e-2 & abs(marker_genes$avg_log2FC) > 1 & marker_genes$diff>0.1,]

marker_genes_filt <- marker_genes_filt[!duplicated(marker_genes_filt$gene),]

ccr <- compareCluster(
  gene ~ cluster,
  data         = marker_genes_filt,
  fun          = "enrichGO",
  OrgDb        = org.Hs.eg.db,
  keyType      = "SYMBOL",
  ont          = "BP",
  pAdjustMethod= "BH",
  pvalueCutoff = 0.05,
  qvalueCutoff = 0.05,
  readable     = TRUE
)

# fix length of terms...
shorten_terms <- function(terms, max_words = Inf, max_chars = Inf) {
  sapply(terms, function(term) {
    words <- strsplit(term, "\\s+")[[1]]
    truncated <- term
    did_trunc <- FALSE
    
    if (!is.infinite(max_words) && length(words) > max_words) {
      truncated <- paste(words[1:max_words], collapse = " ")
      did_trunc <- TRUE
    }
    
    if (!is.infinite(max_chars) && nchar(truncated) > max_chars) {
      cut_sub <- substr(truncated, 1, max_chars)
      spaces <- gregexpr("\\s+", cut_sub)[[1]]
      if (all(spaces == -1)) {
        truncated <- cut_sub
      } else {
        last_space <- tail(spaces, 1)
        truncated <- substr(cut_sub, 1, last_space - 1)
      }
      did_trunc <- TRUE
    }
    
    if (did_trunc) truncated <- paste0(truncated, "...")
    truncated
  }, USE.NAMES = FALSE)
}

ccr@compareClusterResult$Description <- shorten_terms(ccr@compareClusterResult$Description, max_chars = 40)

dotplot(ccr, showCategory=10, font.size = 8) + ggtitle("GO BP enrichment by Archetype")

```

```{r}
# install.packages(c("plotly","geometry","rustytools"))
library(rustytools)
library(plotly)
library(geometry)

set.seed(42)
## 1. Generate toy 3-D data from 4 “ground-truth” archetypes
p  <- 3    # dims
n  <- 5000  # samples
k0 <- 10    # true archetypes

# ground-truth vertices A (3×4)
A <- matrix(runif(p*k0, 0, 1), p, k0)

# random convex combinations S_true (4×200)
alpha    <- matrix(runif(k0*n), k0, n)
S_true   <- sweep(alpha, 2, colSums(alpha), "/")

# data plus a bit of gaussian noise
X <- A %*% S_true + matrix(rnorm(p*n, sd = 0.02), p, n)

## 2. Run PCHA
res <- pcha(X, k0)
#res <- pcha(X, 3)
XC  <- res$XC   # p × k0  (the recovered archetypes in original space)

## 3. Build plotly 3D scatter + hull
pts   <- t(X)      # n×3
verts <- t(XC)     # k0×3

# compute convex‐hull of the archetype points
hull_idx <- convhulln(verts)  # each row is a triangle of vertex‐indices

plot_ly() %>%
  # raw data points
  add_markers(x = pts[,1], y = pts[,2], z = pts[,3],
              marker = list(size = 2, color = 'black'),
              name = "Data") %>%

  # the hull mesh
  add_mesh(x = verts[,1], y = verts[,2], z = verts[,3],
           i = hull_idx[,1]-1,  # convert to zero-based
           j = hull_idx[,2]-1,
           k = hull_idx[,3]-1,
           opacity = 0.3,
           color = 'lightblue',
           name = "Archetype hull") %>%

  # archetype vertices
  add_markers(x = verts[,1], y = verts[,2], z = verts[,3],
              marker = list(size = 6, color = 'red'),
              name = "Archetypes") %>%

  layout(scene = list(
    xaxis = list(title = "Dim 1"),
    yaxis = list(title = "Dim 2"),
    zaxis = list(title = "Dim 3")
  ))

```


```{r}
# install.packages(c("rustytools","plotly","geometry"))
library(rustytools)
library(plotly)
library(geometry)

set.seed(42)
p  <- 3     # 3 dims
n  <- 2000   # samples
k0 <- 4     # true archetypes

# 1) simulate ground‐truth archetypes A and mixing S
A <- matrix(rexp(p * k0, 1), p, k0)
alpha <- matrix(rexp(k0 * n, 1), k0, n)
S_true <- sweep(alpha, 2, colSums(alpha), "/")
X <- A %*% S_true + matrix(rnorm(p * n, sd = 0.05), p, n)

# 2) fit PCHA
res <- pcha(X, k0)

# 3) identify specialists
S <- res$S           # k0 × n
max_w     <- apply(S, 2, max)
best_arch <- apply(S, 2, which.max)
is_spec   <- max_w > 0.85

# 4) get point‐cloud and archetype coordinates
pts <- t(X)          # n × 3
arch <- t(res$XC)    # k0 × 3

# 5) compute convex‐hull faces on the archetypes
hull_faces <- convhulln(arch, options = "Qt")  # a m×3 matrix of 1‐based indices

# 6) build the 3D plot
p3d <- plot_ly() %>%
  # point cloud, colored by specialist or not  
  add_markers(x = pts[,1], y = pts[,2], z = pts[,3],
              color = ifelse(is_spec, paste0("Arch", best_arch), "other"),
              colors = c("red","blue","green","purple","grey80"),
              marker = list(size = 3),
              hoverinfo = 'none') %>%
  # overlay the convex‐hull mesh of the archetypes
  add_mesh(x = arch[,1], y = arch[,2], z = arch[,3],
             i = hull_faces[,1]-1,   # plotly wants 0‐based
             j = hull_faces[,2]-1,
             k = hull_faces[,3]-1,
             opacity = 0.3,
             color = 'black',
             name = 'Archetype hull') %>%
  # add the archetype vertices
  add_markers(x = arch[,1], y = arch[,2], z = arch[,3],
              marker = list(size=6, color='black', symbol='diamond'),
              name = 'Archetypes') %>%
  layout(scene = list(
    xaxis = list(title = "Dim 1"),
    yaxis = list(title = "Dim 2"),
    zaxis = list(title = "Dim 3")
  ))

p3d



```
